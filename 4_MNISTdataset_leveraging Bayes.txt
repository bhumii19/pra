#Develop a classification system for handwritten digit recognition using the MNIST dataset, leveraging Bayes' Decision Theory to optimize decision-making and minimize classification error
# 1. Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# 2. Load MNIST Dataset (using sklearn's fetch_openml or a similar dataset for simplicity)
from sklearn.datasets import fetch_openml

mnist = fetch_openml('mnist_784', version=1)
df = mnist.frame
X = df.drop('class', axis=1)  # Features
y = df['class']  # Target

# 3. EDA: Check for nulls, visualize first few images and class distribution
print(f"Shape of dataset: {df.shape}")
print("\nMissing values:\n", df.isnull().sum())

# Distribution of the target classes
plt.figure(figsize=(8, 4))
sns.countplot(y=y)
plt.title("Distribution of MNIST Digits")
plt.xlabel("Digit")
plt.ylabel("Frequency")
plt.show()

# Visualizing a few random images from the dataset
fig, axes = plt.subplots(1, 5, figsize=(10, 3))
for ax, index in zip(axes, np.random.randint(0, len(X), 5)):
    ax.imshow(X.iloc[index].values.reshape(28, 28), cmap='gray')
    ax.set_title(f"Label: {y.iloc[index]}")
    ax.axis('off')
plt.show()

# 4. Preprocessing: Scaling features (since Naive Bayes assumes Gaussian distribution of features)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 5. Dimensionality Reduction (optional step for better performance with Naive Bayes)
pca = PCA(n_components=50)
X_pca = pca.fit_transform(X_scaled)

# 6. Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# 7. Train Gaussian Naive Bayes Model (Naive Bayes assumes Gaussian distribution of data)
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)

# 8. Make Predictions
y_pred = nb_model.predict(X_test)

# 9. Evaluation: Accuracy, Classification Report, and Confusion Matrix
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
plt.figure(figsize=(10, 8))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()
